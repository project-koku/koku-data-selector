import datetime
import calendar
import json
import logging
import os
import uuid
import requests
import pandas as pd
from datetime import timedelta
from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient, ContainerClient

import azure.functions as func

app = func.FunctionApp()

def extract_vcpus(info):
    # used to extract vcpu details
        try:
            json_data = json.loads(info)
            if isinstance(json_data, dict):
                return json_data.get('vcpus')
        except (json.JSONDecodeError, TypeError, ValueError):
            return None
        return None

def extract_com_redhat_rhel(tags):
    # used to find rhel metering tags
    try:
        json_data = json.loads(tags)
        if isinstance(json_data, dict):
            return json_data.get('com_redhat_rhel') is not None
    except (json.JSONDecodeError, TypeError, ValueError):
        return False
    return False

def translate_columns(df):
    # Normalise required columns across differing azure report types
    column_translation = {        
        "billingcurrency": "billingcurrencycode",
        "currency": "billingcurrencycode",
        "instanceid": "resourceid", 
        "instancename": "resourceid",
        "pretaxcost": "costinbillingcurrency",
        "product": "productname",
        "resourcegroupname": "resourcegroup",
        "subscriptionguid": "subscriptionid",
        "servicename": "metercategory",
        "usage_quantity": "quantity"
        }
    df.rename(columns=column_translation, inplace=True)
    return df

def rhel_filtering(df):
    # Set constants
    tags = "tags"
    additionalinfo = "additionalinfo"
    vcpus = "vcpus"
    rhel_tag = "com_redhat_rhel"

    # filtering specific for rhel metering
    # Filter on Virtual machines
    df = df[(df["metercategory"].astype(str).str.contains("Virtual Machines"))]

    # Set additional info and tags columns to lower for easier filtering
    df[additionalinfo] = df[additionalinfo].str.lower()
    df[tags] = df[tags].str.lower()

    # Extract line items with vcpus and com_redhat_rhel tags
    df[vcpus] = df[additionalinfo].apply(extract_vcpus)
    df[rhel_tag] = df[tags].apply(extract_com_redhat_rhel)

    # Convert 'vcpus' to numeric, forcing errors to NaN (in case of any non-numeric values)
    df[vcpus] = pd.to_numeric(df[vcpus], errors='coerce')

    # Filter rows where 'vcpus' is not null and greater than 0 and 'com_redhat_rhel' is True
    filtered_df = df[df[vcpus].notnull() & (df[vcpus] > 0) & df[rhel_tag]]

    # Drop the 'com_redhat_rhel' and 'vcpus' columns as they are no longer needed and return the filtered data set
    return filtered_df.drop(columns=[rhel_tag, vcpus])

def hcs_filtering(df):
    # Set constants:
    ptype = 'publishertype'
    mscategory = 'metersubcategory'
    pname = 'publishername'

    # Filtering specific to Hybrid Commited Spend
    return df.loc[((df[ptype] == "Azure") & (df[mscategory].astype(str).str.contains("Red Hat"))) | ((df[ptype] == "Marketplace") & ((df[pname].astype(str).str.contains("Red Hat")) | (((df[pname] == "Microsoft") | (df[pname] == "Azure")) & (df[mscategory].astype(str).str.contains("Red Hat")))))]

# The cron schedule passed in here needs to match what you set when creating the function.
@app.timer_trigger(schedule="0 9 * * *", arg_name="myTimer", run_on_startup=False, use_monitor=False) 
def timer_trigger(myTimer: func.TimerRequest) -> None:
    utc_timestamp = datetime.datetime.now().replace(
        tzinfo=datetime.timezone.utc).isoformat()

    default_credential = DefaultAzureCredential()

    now = datetime.datetime.now()
    _, num_days = calendar.monthrange(now.year, now.month)
    month_end = now.replace(day=num_days)
    month_start = now.replace(day=1)
    year = now.strftime("%Y")
    month = now.strftime("%m")
    day = now.strftime("%d")

    # Finalized data
    # Appends finalized data to the last day of the previous month

    # month_end = now.replace(day=1) - timedelta(days=1)
    # month_start = month_end.replace(day=1)
    # year = month_start.strftime("%Y")
    # month = month_start.strftime("%m")
    # day = month_start.strftime("%d")

    output_blob_name=f"{year}/{month}/{day}/{uuid.uuid4()}.csv"

    # Required vars to update
    client_id = os.getenv('ClientIdFromVault')                              # C.R.C service account client id
    client_secret = os.getenv('ClientSecretFromVault')                      # C.R.C service account client secret
    integration_id = "CHANGE-ME"                                            # Cost management integration_id
    cost_export_store = "https://CHANGE-ME.blob.core.windows.net"           # Cost export storage account url
    cost_export_container = "CHANGE-ME"                                     # Cost export container
    cost_export_dir = "CHANGE-ME/CHANGE-ME"                                 # Cost export directory/export-name
    filtered_data_store = "https://CHANGE-ME.blob.core.windows.net"         # Filtered data storage account url
    filtered_data_container = "CHANGE-ME"                                   # Filtered data container
    timeformat = "%Y%m%d"
    report_range = f"{month_start.strftime(timeformat)}-{month_end.strftime(timeformat)}"
    report_path = f"{cost_export_dir}/{report_range}"

    # Get container client
    container_client = ContainerClient(cost_export_store, credential=default_credential, container_name=cost_export_container)

    # Get latest report file
    blob_list = container_client.list_blobs(name_starts_with=report_path)
    latest_blob = None
    for blob in blob_list:
        if latest_blob:
            if blob.last_modified > latest_blob.last_modified:
                latest_blob = blob
        else:
            latest_blob = blob

    bc = container_client.get_blob_client(blob=latest_blob)
    data = bc.download_blob()
    blobjct = "/tmp/blob.csv"
    with open(blobjct, "wb") as f:
        data.readinto(f)
    df = pd.read_csv(blobjct)
    # cast columns to lower
    df.columns = df.columns.str.lower()

    # You must uncomment either hcs_filtering OR rhel_filtering depending on your usecase NOT BOTH #
    # If you want to filter different data entirely you will need to build your own *_filtering query #

    # Hybrid Commited Spend filtering #
    # filtered_data = hcs_filtering(df)

    # RHEL subscriptions filtering #
    # filtered_data = rhel_filtering(df)

    # custom filtering basic example #
    # filtered_data = df.loc[(df["publishertype"] == "Marketplace")] 

    # Save new filtered report CSV to filtered container
    filtered_data_csv = filtered_data.to_csv (index_label="idx", encoding = "utf-8")
    blob_service_client_filter = BlobServiceClient(filtered_data_store, credential=default_credential)
    blob_client = blob_service_client_filter.get_blob_client(container=filtered_data_container, blob=output_blob_name)
    blob_client.upload_blob(filtered_data_csv, overwrite=True)

    # Get service account token
    token_url = 'https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token'
    token_headers = {
        'Content-Type': 'application/x-www-form-urlencoded'
    }
    token_data = {
        "client_id": client_id,
        "client_secret": client_secret,
        "grant_type": "client_credentials"
    }
    token_response = requests.post(token_url, headers=token_headers, data=token_data)

    # Post results to console.redhat.com API
    url = "https://console.redhat.com/api/cost-management/v1/ingress/reports/"
    headers = {
        'Authorization': f'Bearer {token_response.json().get("access_token")}',
        'Accept': 'application/json'
    }
    json_data = {"source": integration_id, "reports_list": [f"{filtered_data_container}/{output_blob_name}"], "bill_year": year, "bill_month": month}
    resp = requests.post(url, json=json_data, headers=headers)
    logging.info(f'Post result: {resp.json()}')

    if myTimer.past_due:
        logging.info('The timer is past due!')

    logging.info('Python timer trigger function ran at %s', utc_timestamp)
